1. この質問については、JencoMart のケーススタディを参照してください。
JencoMart のセキュリティチームは、すべてのGoogle Cloud Platform インフラストラクチャが運用リソースと開発リソースの分離した最小限の権限モデルを使ってデプロイされていることを望んでいます。
どのGoogle ドメインとプロジェクト構造をするべきでしょうか？

- [ ] ユーザーを管理するために開発/テスト/ステージング用と本番用の2つのG Suiteアカウントを作成します。各アカウントにはアプリケーションごとに1つのプロジェクトを含める必要があります。
- [ ] ユーザーを管理するために2つのG Suiteアカウントを作成します。1つはすべての開発アプリケーション用の単一プロジェクト、もう1つはすべての本番アプリケーション用の単一プロジェクトです。
    - 最小限の権限モデル：G Suite アカウントは企業で1つなのでA,BはNG
- [ ] 1つのG Suiteアカウントを作成して、独自のプロジェクトの各アプリケーションの各段階でユーザーを管理します。
    - 環境ごと(開発/STG/本番)にプロジェクトを分けるべきなのでNG
- [x] 単一のG Suiteアカウントを作成して、開発/テスト/ステージング環境用の1つのプロジェクトと本番環境用の1つのプロジェクトでユーザーを管理します。


2. この質問については、JencoMart のケーススタディを参照してください。
JencoMart がユーザ認証情報データベースをGoogle Cloud Platform に移行して古いサーバをシャットダウンした数日後、新しいデータベースサーバはSSH 接続に応答しなくなりました。アプリケーション サーバーへのデータベース要求は正しく処理しています。
問題を診断するには、次のどの手順を実行する必要がありますか？（回答は3つ）

- [ ] 仮想マシン（VM）とディスクを削除して、新しいVMを作成します。
- [ ] インスタンスを削除し、ディスクを新しいVMに接続して調査します。
    - 削除するのは基本NG
- [x] ディスクのスナップショットを取得し、新しいマシンに接続して調査します。
- [x] マシンが接続されているネットワークの受信ファイア ウォールルールを確認します。
- [ ] 非常に単純なファイアウォールルールを使用してVMを別のネットワークに接続し、調査します。
- [x] トラブルシューティング用にインスタンスのシリアルコンソール出力を印刷し、インタラクティブコンソールをアクティブにして調査します。

  - ”削除する”ことは基本的に良くない(リカバリが効かない)ので、A,BはNG
  - SSH接続できないとき：Firewallがおかしい、ディスクがおかしい、sshdが止まってる、とかが考えられる→C,D
  - シリアルコンソール出力をオンにしたら、起動時のログが見れる。→F


3. この質問については、JencoMart のケーススタディを参照してください。JencoMartは、ユーザー プロファイル ストレージをGoogle Cloud Datastore に、アプリケーションサーバーをGoogle Compute Engine（GCE）に移行することを決定しました。
移行中に既存のインフラストラクチャはデータをアップロードするためにGoogle Cloud Datastore にアクセスする必要があります。
どのサービス アカウント キーの管理戦略を推奨する必要がありますか？

- [ ] オンプレミス インフラストラクチャとGCE 仮想マシン（VM）のサービス アカウント キーをプロビジョニングします。
- [ ] ユーザーアカウントを使用してオンプレミス インフラストラクチャを認証し、VMのサービス アカウント キーをプロビジョニングします。
- [x] オンプレミス インフラストラクチャのサービス アカウント キーをプロビジョニングし、VMにGoogle Cloud Platform（GCP）管理鍵を使用します。
- [ ] GCE / Googleにカスタム認証サービスを展開します。 オンプレミスインフラストラクチャ用のGoogle Kubernetes Engine（GKE）およびVM用のGCP 管理キーを使用します。

  - 移行中に既存のインフラストラクチャ(オンプレ環境)が、Datastoreにアクセスする必要がある→サービスアカウントキーをオンプレ(VM以外？)に発行
  - GCPリソースじゃないもの(オンプレ環境とか、他のクラウドのリソース)には、サービスアカウントキーを発行して、GCPのリソースへの書き込み、読み込みとかを許可する。
  - GCPのリソースを使うなら、基本的にGoogle管理の鍵を使った方がいい。GCEに移行が決まってるアプリケーションサーバ(GCE VM)は、GCP管理鍵を使う。→C


4. この質問については、JencoMart のケーススタディを参照してください。JencoMartは、アジアへのトラフィックを提供するGoogle Cloud Platform 上にアプリケーションのバージョンを構築しました。
JencoMartのビジネスと技術的な目標に対する成功を測定したいと考えています。
どの指標を追跡する必要がありますか？

- [ ] アジアからのリクエストのエラー率。
- [ ] 米国とアジアの待ち時間の違い。
- [ ] アジアからの総訪問数、エラー率、および待ち時間。
- [ ] アジアのユーザーの合計訪問数と平均待ち時間。
- [ ] データベースに存在する文字セットの数。

  - ビジネス要件：アジアにビジネスを展開する
  - 技術要件：アジア地区からのアクセスのレイテンシを短縮する
      →C,Dに絞れる
  - エラー率は言及されてないから不要？なのでD
  - Cのアジアからのっていうのがアジアから北米とかやったらアジアでのビジネス関係ないからNG？無理やりすぎるか


5. この質問については、JencoMart のケーススタディを参照してください。
JencoMartのアプリケーションのGoogle Cloud Platform（GCP）への移行の進行が遅れています。インフラストラクチャは上図です。
スループットを最大化したいと望んでいます。
潜在的なボトルネックは何でしょうか？（回答は3つ）

- [x] スループットを制限する単一のVPNトンネル。
- [ ] このタスクに適さないGoogle Cloud Storage の階層。
- [x] 長距離での操作に適さないコピーコマンド。
- [ ] オンプレミス マシンよりもGCPの仮想マシン（VM）が少ない。
- [x] VMの外部の独立したストレージレイヤー。このタスクには適していません。
- [ ] オンプレミス インフラストラクチャとGCP 間の複雑なインターネット接続。

  - 図:サーバラック - エッヂルータ - Cloud VPN - Cloud Storage / マネージドグループインスタンス(VM)
  A 〇 単一のVPNトンネルなのでスループットが制限されている原因
  B ｘ スループットと関係ない
  C 〇 コピーコマンドとかでデータを移行するのは遅い
  D ｘ 見た感じそんなに少なくないし、少なかったところでスループットは関係ない
  E 〇 オンプレと同じ環境にしてRift&Shiftにしたら移行は早い Storageを外部に独立させると、複雑になって遅れるかも
  F ｘ 単一のVPNなのでインターネット接続は単純


6. この質問については、JencoMart のケーススタディを参照してください。JencoMartは、ユーザープロファイル データベースをGoogle Cloud Platformに移動したいと考えています。
どのGoogle データベースを使用する必要がありますか？

- [ ] Google Cloud Spanner
- [ ] Google BigQuery
- [ ] Google Cloud SQL
- [x] Google Cloud Datastore

- ユーザプロファイルというキーワードからDのDatastore
- ユーザプロファイルとかは大切なデータやから、ACIDトランザクションが必要→A,C,D
- そもそもBigQueryはデータ保管用データベースとして使わないのでBはNG
- おそらくSQLもSpannerもダメではない 一番いいのがDatastoreってことかね


7. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、新しいテスト戦略を設計を望んでいます。
テストカバレッジは他のプラットフォームの既存バックエンドとどのように違うべきでしょうか ?

- [x] テストは従来のアプローチをはるかに超えて拡張する必要があります。
- [ ] 単体テストは必要はなく、エンドツーエンドのテストだけが必要です。
- [ ] リリースが実稼働環境になった後にテストを適用する必要があります。
- [ ] テストにはGoogle Cloud Platform のインフラストラクチャの直接テストを含める必要があります。

  - いくつかのゲームは予想を上回る人気を博したため、世界中のオーディエンス、アプリケーション サーバー、MySQL データベース、分析ツールのスケーリングに関する問題が生じました。
    という記述から、今までの他のゲームと違い、テストの段階で大きめのスケールを想定してテストするべきなのでA
  - B,Cはおすすめできない言い方
  - 他のプラットフォームの既存バックエンドでもGCPの直接テストはやるべきなので、DはNG？


8. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、新しいバックエンドをGoogle Cloud Platform（GCP）にデプロイしました。
バックエンドの新しいバージョンが公開される前に、それらのバックエンドの完全なテストプロセスを作成し、テスト環境を経済的な方法で拡張する必要があります。
プロセスをどのように設計するべきでしょか？

- [x] 本番環境の負荷をシミュレートするために、GCPでスケーラブルな環境を作成します。
- [ ] 既存のインフラストラクチャを使用して、GCP ベースのバックエンドを大規模にテストします。
- [ ] GCP内部のリソースを使用してアプリケーションの各コンポーネントにストレステストを構築し、負荷をシミュレートします。
- [ ] GCP で一連の静的環境を作成し、高/中/低 などのさまざまなレベルの負荷をテストします。

  - Mountkirkはスケーラブル環境を求めてるので、テストでもスケーラブルな環境を用意すべきなのでDはNGでAが〇
  - 各コンポーネントにストレステストを実施するのは経済的ではないのでCはNG
  - 既存のインフラを利用したら、テストにならないと思う。新しいテスト環境を作るべきなのでBはNG


9. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、継続的デリバリーパイプラインの確率を望んでいます。
そのアーキテクチャには、迅速に更新およびロールバックできるように考えている小規模サービスが含まれています。
Mountkirk Gamesには次の要件があります。

サービスは、米国とヨーロッパの複数のリージョンに重複して導入されています。
フロントエンドサービスのみが公開インターネットで公開されます。
サービス群に単一のフロントエンド IPを提供できます。
デプロイメントの成果物は不変です。
どのプロダクトを使用するべきですか？

- [ ] Google Cloud Storage、Google Cloud Dataflow、Google Compute Engine.
- [ ] Google Cloud Storage、Google App Engine、Google Network Load Balancer.
- [x] Google Container Registry、Google Kubernetes Engine、Google HTTP(S) Load Balancer.
- [ ] Google Cloud Functions、Google Cloud Pub/Sub、Google Cloud Deployment Manager.

  - CI/CD:Jenkins,MicroService,Containerとかがキーワード
  - 小規模サービスが含まれている→Kubernetes,AppEngine
  - CI/CD→ContainerRegistry+Kubernetesのコンビ
  - 米国とヨーロッパの複数のリージョンに導入されている→グローバルLB→HTTP(S) Load Balancer
  - 単一のフロントエンドIPが必要→Cloud Load Balancing
  - これらからCが〇
  - Dataflowを使う必要が無いし、GCEやとマイクロサービスを扱いづらいのでAはNG
  - Netowork LBはリージョンしか対応していないのでBはNG
  - Pub/SubとFunctionsを使う必要が無いのでDはNG


10. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesのゲームサーバーは、自動的に適切にスケーリングされません。
先月、新機能を発表しから非常に人気になりました。記録的なユーザー数がサービスを使用しようとしていますが、多くは503 エラーと受け取り、応答時間が非常に遅くなっています。
最初に何を調査するべきでしょうか？

- [ ] データベースがオンラインであることを確認します。
- [x] プロジェクトの割り当てを超えていないことを確認します。
- [ ] 新しい機能コードでパフォーマンスのバグが発生しなかったことを確認します。
- [ ] 負荷テストチームが本番環境に対してツールを実行していないことを確認します。

  - 自動的にスケーリングされないから、容量とかそういう系がアップアップになっている
  - エラーコード503はサーバが過負荷でダウンしてるときとかに起こる
  - データベースがオフラインやとそもそもエラーコードを返せない 503が返ってきてるから、オンラインであるのは自明なのでAはNG
  - サーバの過負荷が問題やから、コードでのバグは確認しても意味ないし、負荷テストを実行したしてないか確認するのも手遅れなのでC,DはNG
  - 割り当てが限界まで来てたら、何とか対処したらサーバの過負荷が減るかもしれないのでBが〇


11. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、分離されたアプリケーション環境を展開するために、再現可能で構成可能なメカニズムを作成する必要があります。
開発者とテスターはお互いの環境とリソースにアクセスできますが、ステージングまたは本番環境のリソースにはアクセスできません。ステージング環境は、本番環境から一部のサービスにアクセスする必要があります。
開発環境をステージングおよび本番環境から分離するにはどうすればよいですか？

- [ ] 開発とテスト用のプロジェクトと、ステージングと本番用のプロジェクトを作成します。
- [ ] 開発とテスト用のネットワークと、ステージングと本番用のネットワークを作成します。
- [ ] 開発用に1つのサブネットワークを作成し、ステージングと本番用に別のサブネットワークを作成します。
- [x] 開発用に1つ、ステージング用に2つ目、本番用に3つ目のプロジェクトを作成します。

  - アプリケーション環境の分離は、プロジェクト単位でやるのでB,CはNG
  - 開発用、STG用、本番用でそれぞれプロジェクトを分けるべきなのでDが〇


12. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、新しいゲーム用にリアルタイムの分析プラットフォームをセットアップしたいと考えています。
新しいプラットフォームは、技術的要件を満たす必要があります。
すべての要件を満たすGoogle プロダクトの組み合わせはどれです？

- [ ] Google Kubernetes Engine、Google Cloud Pub/Sub、Google Cloud SQL.
- [x] Google Cloud Dataflow、Google Cloud Storage、Google Cloud Pub/Sub、Google BigQuery.
- [ ] Google Cloud SQL、Google Cloud Storage、Google Cloud Pub/Sub、Google Cloud Dataflow.
- [ ] Google Cloud Dataproc、Google Cloud Pub/Sub、Google Cloud SQL、Google Cloud Dataflow.
- [ ] Google Cloud Pub/Sub、Google Compute Engine、Google Cloud Storage、Google Cloud Dataproc.

  - 分析プラットフォームの要件
    - 動的スケーリング → 動的スケーリングできるやつ → Dataflow,Kubernetes,Dataproc
      - GCEがNG(?)なのでEがNG
    - 着信データの迅速な処理 → Dataflow , Pub/Sub
    - 遅延したデータの処理 → Pub/Sub
    - 10TB以上の履歴データにクエリできる → BigQuery
      - SQLがNGなのでA,C,DがNG
    - 定期的にアップロードされるデータの処理 → Batch処理 → Dataflow
    - →Bが〇


13.  ★この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、現在の分析および統計レポートモデルから、Google Cloud Platform 上の技術的要件を満たすモデルに移行したいと考えています。
移行計画に含めるべき2つのステップはどれでしょうか？(回答は2つ)

- [x] 現在のバッチ ETL コードをCloud Cloud Dataflow に移行した場合の影響を評価します。
- [x] Google BigQuery のパフォーマンスを向上させるために、データを非正規化するスキーマ移行計画を作成します。
- [ ] 単一のMySQL データベースからMySQL クラスタに移動する方法を示すアーキテクチャ図を作成します。
- [ ] 前のゲームから10 TBの分析データをGoogle Cloud SQL インスタンスにロードし、データセット全体に対してテストクエリを実行して、正常に完了したことを確認します。
- [ ] Google Cloud Armor を統合して、Google Cloud Storage にアップロードされた分析ファイル内のSQL インジェクション攻撃の可能性を防ぎます。

  - DataflowはETLツールなのでAは〇
  - 正規化はテーブルとテーブルを組み合わせて複雑にする作業で、パフォーマンスが少し下がる
  - 非正規化すると、テーブルの構造が単純になるので、パフォーマンスが向上する(軽くなる？)のでBも〇
  - 10TB以上の履歴をクエリできるのが条件で、CloudSQLはNGなのでC,DはNG
  - セキュリティ関係は技術要件に記載がなく関係ないのでDはNG


14.  ★この質問については、Mountkirk Games のケーススタディを参照してください。
会社のMountkirk Gamesのコンピューティングワークロードの技術アーキテクチャを分析および定義する必要があります。
Mountkirk Gamesのビジネス要件と技術的要件を考慮して、何をすべきですか？

- [ ] ネットワーク ロードバランサを作成します。Google Compute Engine プリエンプティブル インスタンスを使用します。
- [ ] ネットワーク ロードバランサを作成します。Google Compute Engine インスタンスを使用します。
- [ ] マネージド インスタンス グループ（MIG）と自動スケーリングポリシーを使用してネットワーク ロードバランサを作成します。Googel Compute Engine プリエンプティブル インスタンスを使用します。
- [x] マネージド インスタンス グループ（MIG）と自動スケーリングポリシーを使用してネットワーク ロードバランサを作成します。Google Compute Engine インスタンスを使用します。

  - 動的スケーリングが必要なのでA,BはNG
  - 本番にプリエンプティブルを使うのはよろしくないのでCはNG


15. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、パブリック クラウドと技術の改善が利用可能になったときにそれを活用するために、将来のためのソリューションを設計したいと考えています。
どのステップを取るべきですか？(回答は2つ)

- [ ] 将来のユーザー行動を予測するための機械学習モデルを訓練するために利用できるように、現在可能な限り多くの分析データとゲームアクティビティデータを保存します。
- [ ] ゲームバックエンド アーティファクトをコンテナイメージにパッケージ化し、Google Kubernetes Engine 上で実行することでゲームアクティビティに応じてスケールアップやスケールダウンできる可用性を改善します。
- [x] Jenkins とSpinnaker を使って CI/CD パイプラインのセットアップを行い、カナリアの展開を自動化し、開発速度を改善します。
- [ ] 追加のプレーヤーデータをデータベースに保存する必要がある新しいゲーム機能を追加する際のダウンタイムを短縮するために、スキーマのバージョン管理ツールを採用します。
- [x] Linux 仮想マシンに週単位のローリング メンテナンス プロセスを実装して、重要なカーネルパッチとパッケージアップデートを適用し、ゼロデイの脆弱性のリスクを軽減します。

  - AはなんでNGなんかよくわからん データ保存するならもう早く移行しちゃった方がいいってことかね？
  - MachineLearningEngine使えるようになったときに有用やからおそらくA,Cがあってると思う
  - バックエンドを動的スケールに対応させても、バックエンドやからゲームアクティビティに応じてスケールはしないのでは？なのでBはNG?しかもKubernetesEngine上で実行ってそれもうパブリッククラウド使ってますやん？
  - なんとなくCは必要な気がするのでCが〇
  - Dはパブリッククラウドを使うことによる技術改善と関係ないからNG
  - ゼロデイ(攻撃):セキュリティに脆弱性が発見されたとき、そのパッチが導入されるよりも前に脆弱性を突かれて攻撃されること
  - 定期メンテ導入でゼロデイリスクを抑えるのはいいことなのでDは〇？


16. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Gamesは、モバイルネットワークの遅延の変化に対する分析プラットフォームの復元力をテストする方法を設計したいと考えています。
どうすればいいですか？

- [ ] ゲーム分析プラットフォームに障害注入ソフトウェアを導入し、モバイルクライアントの分析トラフィックに遅延を追加できます。
- [ ] Google Compute Engine 仮想マシンの携帯電話エミュレーターから実行できるテストクライアントを構築し、世界中のGoogle Cloud Platform リージョンで複数のコピーを実行して、現実的なトラフィックを生成します。
- [x] モバイルデバイスからアップロードされた分析ファイルの処理を開始する前に、ランダムな遅延を導入する機能を追加します。
- [ ] プレーヤーのモバイルデバイスで実行し、世界中のGoogle Cloud Platform リージョンで実行されている分析エンドポイントから応答時間を収集するゲームのオプトインベータ版を作成します。

  - 遅延に対して分析プラットフォームが復元する力をテストしたいので、分析ファイルの処理開始前にランダムな遅延を入れるとGood
  - 他の選択肢は何言ってるかよくわからん


17. この質問については、Mountkirk Games のケーススタディを参照してください。
Mountkirk Games のデータベース ワークロードの技術アーキテクチャーを分析および定義する必要があります。
ビジネス要件と技術的要件を考慮してください。
どうすればいいですか？

- [ ] 時系列データにはGoogle Cloud SQL を使用し、履歴データクエリにはGoogle Cloud Bigtable を使用します。
- [ ] Google Cloud SQL を使用してMySQLを置き換え、Google Cloud Spanner を使用して履歴データクエリを実行します。
- [x] Google Cloud Bigtable を使用してMySQLを置き換え、Google BigQuery を履歴データクエリに使用します。
- [ ] 時系列データにGoogle Cloud Bigtable を使用し、トランザクションデータにGoogle Cloud Spanner を使用し、履歴データクエリにGoogle BigQuery を使用します。

  - 履歴データクエリにはBigQueryが最適なのでA,BはNG
  - MountkirkはMySQLのスケーラビリティに問題を抱えていた
  - MySQLはユーザーの携帯端末からデータをETLツールで持ってきて分析するためのデータベースとして使ってた
  - このMySQLはBigtableに置き換えられそうなのでCが〇？
  - 技術要件にトランザクションデータベースでユーザプロファイルを保存ってあるからSpannerも必要やと思うんやけど
  - 正解Dじゃね？


18.  ★この質問については、Mountkirk Games のケーススタディを参照してください。
時系列データベースサービスにゲームアクティビティを保存するためのMountkirk の技術的要件を満たすGoogle Cloud ストレージ プロダクトはどれでしょうか？

- [x] Google Cloud Bigtable
- [ ] Google Cloud Spanner
- [ ] Google BigQuery
- [ ] Google Cloud Datastore

  - 時系列はBigtable


19.  ★この質問については、Mountkirk Games のケーススタディを参照してください。
新しいゲームバックエンド プラットフォームのアーキテクチャを担当しています。
ゲームはREST API を介してバックエンドと通信します。
Google のベストプラクティスに従います。
バックエンドをどのように設計すべきか？

- [ ] バックエンドのインスタンス テンプレートを作成します。 すべてのリージョンについて、複数ゾーンのマネージド インスタンス グループ（MIG）にデプロイします。L4 ロードバランサ（TCP プロキシ負荷分散）を使用します。
- [ ] バックエンドのインスタンス テンプレートを作成します。 リージョンごとに、単一ゾーンのマネージド インスタンス グループ（MIG）にデプロイします。 L4 ロードバランサ（TCP プロキシ負荷分散）を使用します。
- [x] バックエンドのインスタンス テンプレートを作成します。 すべてのリージョンについて、複数ゾーンのマネージド インスタンス グループ（MIG）にデプロイします。L7 ロードバランサ（HTTP(S) 負荷分散）を使用します。
- [ ] バックエンドのインスタンス テンプレートを作成します。 リージョンごとに、単一ゾーンのマネージド インスタンス グループ（MIG）にデプロイします。 L7 ロードバランサ（HTTP(S) 負荷分散）を使用します。

  - ゲームバックエンドなので内部LB
  - 内部はTCP/UDP負荷分散とHTTP(S)負荷分散がある
  - 技術要件に動的にスケールアップできるがあるので、マネージドインスタンスグループは複数必要
  - REST APIで通信ってことはURIで制御やからC


20. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthのCTOは、接続された車両の生データを使用して、現場の車がいつ壊滅的な故障を起こすかをおおよそ特定したいと考えています。
そのために、ビジネス アナリストが車両データを一元的に照会できるようにする必要があります。
どのアーキテクチャをおすすめしますか？

A. IoT → LB → Container → Pub/Sub → Dataflow → Big Query → Analysys
B. IoT → App Engine → Pub/Sub → Dataflow → Big Query → Analysys
C. IoT → LB → Container → Pub/Sub → Dataflow → Cloud SQL → Analysys
D. IoT → App Engine → Pub/Sub → Dataflow → Cloud SQL → Analysys

- [x] a
- [ ] b
- [ ] c
- [ ] d

  - BigQuery or SQL , LB or AppEngine
  - IoTCoreは基本的にMTQQかHTTPのLBに接続するので、AppEngineには接続できない。よってB,DはNG
  - IoTとかから収集するデータはビッグデータなので、SQLよりはBigQueryに取り込んで、分析に使うのでAが正解


21. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthの開発チームは、ビジネス要件を満たすAPIを作成したいと考えています。
開発チームには、カスタム フレームワークを作成するのではなく、ビジネス バリューに開発作業を集中させる必要があります。
どの方法を良いでしょうか？

- [x] Google App Engine をGoogle Cloud Endpoints で使用します。ディーラーおよびパートナー向けのAPIに注力します。
- [ ] JAX-RS Jersey Java ベースのフレームワークでGoogle App Engine を使用します。一般向けのAPIに注力します。
- [ ] Swagger（Open API 仕様）フレームワークでGoogle App Engineを使用します。一般向けのAPIに注力します。
- [ ] Django Python コンテナでGoogle Container Engine を使用します。 一般向けのAPIに注力します。
- [ ] Swagger（Open API 仕様）フレームワークを備えたTomcat コンテナでGoogle Container Engine を使用します。 ディーラーおよびパートナー向けのAPIに注力します。

  - APIにはCloudEndpointを使う
  - マネージドサービスを使うことで、インフラの構築とかに手間を費やさなくて済む → 開発作業に集中できる


22. この質問については、TerramEarth のケーススタディを参照してください。
開発チームは、車両データを取得するための構造化APIを作成しました。
開発チームは、この車両イベントデータを使用するディーラー用のツールをサードパーティが開発できるようにしたいと考えています。
このデータに対する委任された承認をサポートする必要があります。
どうすればいいでしょうか？

- [x] OAuth と互換のあるアクセス制御システムを構築または活用します。
- [ ] SAML 2.0 SSO 互換性を認証システムに組み込みます。
- [ ] パートナーシステムのソース IP アドレスに基づいてデータアクセスを制限します。
- [ ] 信頼できるサードパーティに提供できる各ディーラーの二次認証情報を作成します。

  - 問題文が難解なので整理
    - 開発チームは車両データを取得するAPIを作成した
    - ディーラーは車両データを使ってなんやかんやしたいので、ツールを作ってほしい
    - サードパーティは開発チームが作成したAPIを利用した、車両データを取得するツールを開発できるようにしてほしい
      - ↑APIへの認証がないとAPI利用できない
    - 認証についてどうするか
  - APIへのアクセスには認証が必要
  - 認証には、アプリケーションの識別と、プリンシパルの識別(ユーザアカウントかサービスアカウントかどうか)の2つが必要
  - APIは、登録されたアプリケーションのリクエストのみ利用を受け付ける
  - 開発チームの作成した構造化APIに、エンドユーザ(今回の場合サードパーティ)が開発したツール(アプリケーション)を登録したら、認証されて利用可能になる。
  - 認証に使えるもの：APIキー、OAuth2.0資格情報、サービスアカウントキー
    →OAuthを使ったら、アプリケーションとプリンシパル両方識別できる
    →OAuthと互換のあるアクセス制御システムならいい感じなのでAが〇
  - OAuth以外は認証に使えないのでNG
  - SAML:Security Assertion Markup Language
  - SSO:Single Sign On


23. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthは、現場にある2,000万台すべての車両をパブリック クラウドに接続することを計画しています。
これにより、2,000万600 バイトレコード/秒で40 TB/時のボリューム増加が予想されています。
データの取り込みをどのように設計する必要がありますか？

- [ ] 車両は、Google Cloud Storage にデータを直接書き込みます。
- [x] 車両は、Google Cloud Pub/Sub にデータを直接書き込みます。
- [ ] 車両は、データをGoogle BigQuery に直接ストリーミングします。
- [ ] 車両は、引き続き既存のシステム（FTP）を使用してデータを書き込みます。

  - FTPを利用すると3週間かかってダメってことなのでDはNG
  - Cloud Storage は静的データの保存に適していて、Bigデータの保存には向いてないのでAはNG
  - ストリーミング処理に対応しているBかC
  - BigQueryのストリーミングは、Avro,CSV,JSONなどのファイル形式に対応している
  - BigQueryでは、CSV,JSONは、gzipで圧縮されたファイルより非圧縮のファイルの方が高速に処理できる。ちな、Avroはどっちでも高速
  - データを圧縮せず、ストリーミングでBigQueryにデータを取り込むと一番いい
  - ただし、BigQueryのストリーミングは1秒間に1GBの上限があるのでCはNG
  - Pub/Subのトピックに取り込んで、DataFlowなどからBigQueryに送るのがいいのでBが〇


24. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthのビジネス要件を分析したところ、ダウンタイムを削減し、顧客の部品の待ち時間を短縮することで、時間の大半を節約できることがわかりました。
3週間分の集約レポート時間の短縮に重点を置くことを決定しました。
会社のプロセスにどの変更するべきでしょうか？

- [ ] CSV形式からバイナリ形式に移行、FTPからSFTP トランスポートに移行を行い、メトリックの機械学習分析を開発します。
- [ ] FTPからストリーミング トランスポートへの移行、CSVからバイナリ形式への移行して、およびメトリックの機械学習分析を開発します。
- [x] フリートのセルラー接続を80％に増やし、FTPからストリーミング トランスポートに移行し、メトリックの機械学習分析を開発します。
- [ ] FTPからSFTPトランスポートに移行し、メトリックの機械学習分析を開発し、ディーラーのローカル在庫を一定の要因で増やします。

  - CSV形式からバイナリ形式に移行  : 時間短縮にはならない むしろ、CSVのほうが早そう
  - FTPからSFTPへの移行            : セキュリティを向上させるには良いけど時間短縮にならない
  - メトリックの機械学習分析を開発 : ダウンタイム短縮に対して効果的
  - セルラー接続を80%に増やす      : 2000万台中20万台(1%)はセルラー接続してて、データを直接収集できて、速い
                                     それを80%に増やすということなので、時間短縮につながる
  - FTPからストリーミングへ移行    : バッチ処理よりストリーミング処理のほうが時間短縮になる
  - ディーラーのローカル在庫の増加 : データの取得時間短縮には関係ない

  - A は不正解です。機械学習による分析はダウンタイム短縮のための良い手段ですが、形式や転送の移行は直接的には役立ちません。
  - B は不正解です。機械学習による分析はダウンタイム短縮のための良い手段であり、ストリーミングに移行することでこの分析に使用する情報の鮮度を向上させることができますが、形式の変更は直接的には役立ちません。
  - C が正解です。携帯電話回線に接続することで、マシンがメンテナンスに入っているときに収集される現在のデータよりも、分析に使用するデータの鮮度が大幅に向上します。定期的な FTP 転送の代わりにストリーミング転送を使用すると、rationale ループをさらに短縮できます。機械学習はメンテナンス ワークロードの予測に理想的です。
  - D は不正解です。機械学習による分析はダウンタイム短縮のための良い手段ですが、その他の変更は直接的には役立ちません。


25. この質問については、TerramEarth のケーススタディを参照してください。
Google Cloud Platform の利用を続ける結果、TerramEarthの設備投資（または資産計上）のどれが大きく変化するでしょうか？

- [ ] 運用コスト/設備投資の割り当て、LANの変更、容量計画。
- [x] キャパシティプランニング、TCO計算、運用コスト/設備投資の割り当て。
- [ ] キャパシティプランニング、使用率測定、データセンターの拡張。
- [ ] データセンターの拡張、TCO計算、使用率測定。

- CapEx / OpEx             : 運用投資、設備投資のこと
- TCO計算                  : Total Cost of Ownership の略で コンピュータの導入や、管理維持に関わるすべてのコストの総額をさす。
- キャパシティプランニング : アプリケーションのニーズを満たすために必要なハードウェアおよびソフトウェアの構成を決定するプロセスのこと

  - クラウド移行によって
    - 設備投資(固定費用)が減って運用投資(変動費)の増加が見込まれる(従量課金になっていく)
    - TCOが大きく変化する(おそらく、コスト削減の方向で)
    - キャパシティプランニングが大きく変わる(クラウドネイティブな構成になる)
  - のでBが〇

  - クラウドに移行することでデータセンターは縮小することになるので、C,DはNG
  - クラウドに移行することでLANの管理もしないですむようになるので、AはNG


26.  この質問については、TerramEarth のケーススタディを参照してください。
データ取得を高速化するために、より多くの車両がセルラー接続にアップグレードされ、ETLプロセスにデータを送信できるようになります。
現在のFTP プロセスは頻繁にエラーを起こしやすく、接続に失敗するとファイルの初めからデータ転送を再開します。
ソリューションの信頼性を向上させ、セルラー接続でのデータ転送時間を最小限に抑える必要があります
どうすれば良いでしょうか？

- [ ] FTP サーバのGoogle Container Engine クラスタを1つ使用します。データを複数地域バケットに保存し、バケット内のデータを使用してETL プロセスの実行します。
- [ ] 異なる地域にあるFTP サーバを実行する複数のGoogle Container Engine クラスタを使用します。データを米国、EUおよびアジアの複数地域バケットに保存し、バケット内のデータを使用してETL プロセスを実行します。
- [ ] HTTP（S）上でGoogle API を使用して、米国、EU、アジア内のさまざまなGoogle Cloud Multi-Regional Storage バケット ロケーションにファイルを直接転送します。バケット内のデータを使用してETL プロセスを実行します。
- [x] HTTP（S）上でGoogle API を使用して、米国、EU、アジア内の別のGoogle Cloud Regional Storage バケット ロケーションにファイルを直接転送します。ETL プロセスを実行して、各地域バケットからデータを取得します。

  - FTPを使うのがクッソ遅いからFTPはやめるべきなので、A,BはNG
  - 転送時間(レイテンシ)とか、帯域幅は、リージョナルに保存するのが一番いいのでDが〇
  - データを集めてきて保存はリージョナルで、CDN的に使うのがマルチリージョナルってことかね。
  ```
  ロケーションに関する留意事項

  レイテンシ、可用性、帯域幅、コストのバランスがとれた場所が適切なロケーションになります。
  同一リージョンにグループ化されている分析パイプラインなどのデータ コンシューマでレイテンシ、ネットワーク帯域幅を最適化する場合は、リージョンのロケーションを使用します。
  リージョンと同等のパフォーマンスに加えて地理的な冗長性による高可用性が必要な場合は、デュアルリージョンを使用します。
  Google ネットワークの外部にあり、広域に分散しているデータ コンシューマにコンテンツを配信する場合や、地理的冗長性による高可用性を使用する場合は、マルチリージョンを使用します。
  通常、便利で、データを利用する大半のユーザーが含まれるロケーションにデータを格納する必要があります。
  各ロケーションのストレージ費用については、データ ストレージの料金表をご覧ください。
  ```


27. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthの2,000万台の車は世界中に散らばっています。
車両の位置に基づいて、テレメトリデータはGoogle Cloud Storage（GCS）のリージョンバケット（米国、欧州、アジア）に保存されています。
CTOは、なぜ車が100キロ走行した後に故障しているのかを判断するために、生のテレメトリデータに関するレポートの作成を望んでいます。
このジョブをすべてのデータに対して実行します。
このジョブを実行するための最も費用対効果の高い方法はどれでしょうか？

- [ ] すべてのデータを1つのゾーンに移動し、Google Cloud Dataproc クラスタを起動してジョブを実行します。
- [ ] すべてのデータを1つのリージョンに移動してから、Google Cloud Dataproc クラスタを起動してジョブを実行します。
- [ ] 各リージョンでクラスタを起動して未処理データを前処理および圧縮し、データをマルチリージョンのバケットに移動し、Google Cloud Dataproc クラスタを使用してジョブを仕上げます。
- [x] 各リージョンでクラスタを起動して未処理データを前処理および圧縮し、データをリージョンバケットに移動し、Google Cloud Dataproc クラスタを使用してジョブを仕上げます。

- 未加工- 非圧縮のデータは容量がでかいので、いきなりDataproc処理すると費用が高くなるのでA,BはNG
- Q26同様、データを集めてきて保存はリージョナルがいいのでD


28. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthは、接続されているすべてのトラックにサーバとセンサーを搭載し、遠隔測定データを収集しています。
来年にこのデータを使用して機械学習モデルをトレーニングしたいと考えています。 またコストを削減しながら、このデータをパブリック クラウドに保存したいとも考えています。
何をすれば良いでしょうか？

- [ ] トラックのコンピューターに1時間ごとのスナップショットでデータを圧縮させ、Google Cloud Nearline Storage バケットに保存します。
- [ ] 遠隔測定データを、データを圧縮するストリーミングデータ フロージョブにリアルタイムでプッシュし、Google BigQuery に保存します。
- [ ] データを圧縮するストリーミングデータ フロージョブにリアルタイムで遠隔測定データをプッシュし、Google Cloud Bigtable に保存します。
- [x] トラックのコンピューターに1時間ごとのスナップショットでデータを圧縮させ、Google Cloud Coldline Storage バケットに保存します。

- 機械学習～～で、すぐBigQueryって答えたくなるとこやけど、まずはコストを削減しながら、パブリッククラウドに保存を第一に考える。
- 「来年に」とあるので保存期間は360日 → ColdLineの保存期間目安 Nearlineは30日
- コスト削減ならColdLine一択なのでD


29. この質問については、TerramEarth のケーススタディを参照してください。
農業部門は完全自動運転車の実験をしています。
車両の運用中に強力なセキュリティを強化するためのアーキテクチャが必要です。
どのアーキテクチャを検討する必要がありますか？(回答は2つ)

- [x] 車両上のモジュール間のすべてのマイクロサービスコールを信頼できないものとして扱います。
- [ ] 安全なアドレス空間を確保するために、接続にIPv6 が必要です。
- [x] トラステッドプラットフォームモジュール（TPM）を使用し、起動時にファームウェアとバイナリを確認します。
- [ ] 関数型プログラミング言語を使用して、コード実行サイクルを分離します。
- [ ] 冗長性のために複数の接続サブシステムを使用します。
- [ ] チップを分離するために、車両の駆動電子機器をファラデーケージに入れます。

  - A は正解です。この方法では、モジュール間の中間者攻撃などによるハッキングへの耐性を高めることができるため、システムのセキュリティが向上します。
  - B は不正解です。IPv6 ではシステムのスケーラビリティの向上と簡略化を図ることができますが、車両走行中の安全性は促進されません。
  - C は正解です。この方法では、悪意のある人物によるルートキットやその他の種類の改ざんなどのハッキングへの耐性を高めることができるため、システムのセキュリティが向上します。
  - D は不正解です。単に関数型プログラミング言語を使用しても、より安全なレベルの実行分離は保証されません。この選択により安全性が改善したとしても、偶発的なものにすぎません。
  - E は不正解です。この方法はシステムの耐久性を改善はしますが、車両走行中の安全性は促進されません。
  - F は不正解です。この方法はシステムの耐久性を改善はしますが、車両走行中の安全性は促進されません。


30. この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthの各車両は、環境条件に応じて油圧などの運転パラメータを調整して効率を向上させることができます。
主な目標は、携帯電話と未接続の2,000万台すべての車両の現場での作業効率を向上させることです。
この目標を達成するには何をすれば良いでしょうか？

- [ ] データのパターンを検査し、動作調整を自動的に行うルールを使用してアルゴリズムを作成します。
- [x] すべての動作データをキャプチャし、理想的な動作を特定する機械学習モデルをトレーニングし、ローカルで実行して自動的に動作調整を行います。
- [ ] スライディング ウィンドウでGoogle Cloud Dataflow ストリーミング ジョブを実装し、Google Cloud Messaging（GCM）を使用して運用上の調整を自動的に行います。
- [x] すべての操作データをキャプチャし、理想的な操作を識別する機械学習モデルをトレーニングし、Google Cloud Machine Learning（ML）プラットフォームでホストして、操作を自動的に調整します。

  - MLを使うべきなのでA,CはNG
  - 携帯電話と未接続の車両はGoogleCloudMachineLarningに接続できないので、Bが〇?
  - MLプラットフォームでホストしないとそもそもML系の作業できないしDが〇?


31. この質問については、TerramEarth のケーススタディを参照してください。
欧州連合の一般データ保護規則（GDPR）に準拠するために、TerramEarthは、個人データが含まれる36か月後にヨーロッパの顧客から生成されたデータを削除する必要があります。
新しいアーキテクチャでは、このデータはGoogle Cloud Storage とGoogle BigQuery の両方に保存されます。
何をするべきでしょうか？

- [ ] 欧州連合 データ用のGoogle BigQuery テーブルを作成し、テーブルの保存期間を36ヶ月に設定します。 Google Cloud Storageの場合、gsutil を使用して、36ヶ月のage 条件でDelete アクションを使用するライフサイクル管理を有効にします。
- [ ] 欧州連合 データ用のGoogle BigQuery テーブルを作成し、テーブルの保存期間を36ヶ月に設定します。Googel Cloud Storageの場合、36ヶ月のage 条件の場合、gsutil を使用してSetStorageClass to NONE アクションを作成します。
- [x] 欧州連合 データ用のGoogle BigQuery タイムパーティションテーブルを作成し、パーティション期限を36ヶ月に設定します。Google Cloud Storageの場合、gsutilを使用して、36ヶ月のage 条件でDelete アクションを使用するライフサイクル管理を有効にします。
- [ ] 欧州連合 データ用のGoogle BigQuery タイムパーティションテーブルを作成し、パーティション期間を36ヶ月に設定します。Google Cloud Storageの場合、gsutilを使用して、36ヶ月のage 条件でSetStorageClass to NONE アクションを作成します。

  - 標準テーブル or パーティション分割テーブル
  - Delete or Class to NONE
  - テーブルの保存期間を設定すると、テーブルの作成時から36か月でテーブル自体が削除されてしまう
  - つまり、テーブルに入れて36か月たってないデータも削除される可能性がある
  - 取り込み時間パーティション分割テーブルを使うと、データの到着日から、36か月経ったデータだけ削除できるので、Cが〇
  - Storageに関しては、Class to NONEなんてオプションはない(Set to standard,near,cold,archive か Delete)


32.  この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthは、データファイルをGoogle Cloud Storage（GCS）に保存することを決定しました。
GCS のライフサイクル ルールを設定して、1年間のデータを保存しますが、ファイルス トレージのコストを最小限に抑える必要があります。
どうすればいいでしょうか？

- [x] 「 Age：30、Storage Class：Standard、アクション：Coldline に設定」でGCS のライフサイクル ルールを作成し、Age 条件で2つ目のGCS ライフサイクル ルールを作成します。
Age：365、Storage Class：Coldline、アクション：Delete
- [ ] 「 Age：30、Storage Class：Cloudline、アクション：Nearline に設定」でGCS のライフサイクル ルールを作成し、Age 条件で2つ目のGCS ライフサイクル ルールを作成します。
Age：91、Storage Class：Cloudline、アクション：Nearline に設定
- [ ] 「 Age：90、Storage Class：Standard、アクション：Nearline に設定」でGCS のライフサイクル ルールを作成し、Age 条件で2つ目のGCS ライフサイクル ルールを作成します。
Age：91、Storage Class：Nearline、アクション：Coldline に設定
- [ ] 「 Age：30、Storage Class：Standard、アクション：Coldline に設定」でGCS のライフサイクル ルールを作成し、Age 条件で2つ目のGCS ライフサイクル ルールを作成します。
Age：365、Storage Class：Nearline、アクション：Delete

  - 一年間保存なので、Age:365でDeleteじゃないとあかんからB,CはNG
  - Dは条件1でColdLine指定してるのに条件2ではNearLine指定しててガバいのでNG


33.  ★この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthのデータ ウェアハウスに、信頼性と拡張性に優れたGCP ソリューションを実装する必要があります。
TerramEarthのビジネス要件と技術的要件を考慮してください。
何をするべきでしょうか？

- [x] 既存のデータ ウェアハウスをGoogle BigQueryに置き換え、パーティション分割テーブルを使用します。
- [ ] 既存のデータ ウェアハウスを96個のvCPUを持つGoogle Compute Engine インスタンスに置き換えます。
- [ ] 既存のデータ ウェアハウスをGoogle BigQueryに置き換え、外部データソース（フェデレーション データソース）を使用します。
- [ ] 既存のデータ ウェアハウスを96個のvCPUのGoogle Compute Engine インスタンスに置き換え、 32個のvCPUのGoogle Compute Engine プリエンプティブル インスタンスを追加します。

  - データウェアハウスはBigQueryを使う。
  - パーティション分割テーブル：テーブルを、取り込み時間とか、タイムスタンプ、IDとかのパーティションで区切ったテーブルの事
  - フェデレーションデータソース：CloudStorageとかBigtableに保存されたデータソース
  - パーティション分割テーブルを使うと、パフォーマンスが向上する
  - フェデレーションデータソースは、いろいろと制約事項がある
  - よってAが〇


34.  ★この質問については、TerramEarth のケーススタディを参照してください。
すべての入力データをGoogle BigQuery に書き込む新しいアーキテクチャが導入されました。
データが汚れていることに気付き、コストを管理しながら、毎日自動化されたデータ品質を確保したいと考えています。
何をするべきでしょうか？

- [ ] Google Cloud Dataflow ストリーミング ジョブを設定し、取り込みプロセスでデータを受信します。 Google Cloud Dataflow パイプラインのデータをクリーンアップします。
- [ ] Google BigQuery からデータを読み取り、クリーンアップするGoogle Cloud Functions を作成します。Google Compute EngineインスタンスからGoogle Cloud Functions をトリガーします。
- [ ] Google BigQuery のデータにSQL文を作成し、ビューとして保存します。 ビューを毎日実行し、結果を新しいテーブルに保存します。
- [x] Google Cloud Dataprep を使用して、Google BigQuery テーブルをソースとして構成します。 データを消去するために毎日のジョブをスケジュールします。

  - データが汚れているからクリーンアップ、自動化、スケージュールとかのキーワードから、Dataprepを使う
  - BigQueryでクエリをかけても、データ自体はクリーンアップできないのでB,CはNG
  - Dataflowもデータをストリーミングかバッチ処理するだけなので、データのクリーンアップはできない よってAもNG


35.  ★この質問については、TerramEarth のケーススタディを参照してください。
TerramEarthの技術的要件を考慮した場合、Google Cloud Platform で予想外の車両のダウンタイムをどのように削減するべきですか?

- [x] Google BigQuery をデータ ウェアハウスとして使用します。すべての車両をネットワークに接続し、Google Cloud Pub/Sub とGoogle Cloud Dataflow を使用してデータをGoogle BigQuery にストリーミングします。分析とレポートにGoogle データポータル（データスタジオ） を使用します。
- [ ] Google BigQuery をデータ ウェアハウスとして使用します。すべての車両をネットワークに接続し、gcloud を使用してgzip ファイルをGoogle Cloud Multi-Regional Storage バケットにアップロードします。 分析とレポートにGoogle データポータルを使用します。
- [ ] Google Cloud Dataproc Hive をデータ ウェアハウスとして使用します。gzip ファイルをGoogle Cloud Multi-Regional Storage バケットにアップロードします。gcloud を使用して、このデータをGoogle BigQuery にアップロードします。分析とレポートにGoogle データポータルを使用します。
- [ ] Google Cloud Dataproc Hive をデータウェアハウスとして使用します。パーティション化されたHive テーブルにデータを直接ストリーミングします。Pig スクリプトを使用してデータを分析します。

  - データウェアハウスはBigQueryを使う。
  - CloudStorageへのアップロードはgsutilなので、gcloudとしてるBはNG


36.  ★この質問については、TerramEarth のケーススタディを参照してください。
携帯電話ネットワークに接続されている20万台の車両のデータを取り込むための新しいアーキテクチャを設計するよう求められます。
Google のベストプラクティスに従ってください。
TerramEarthの技術的要件を考慮すると、データの取り込みにはどのコンポーネントを使用する必要がありますか。

- [ ] SSL Ingress を使用したGoogle Kubernetes Engine。
- [x] 公開鍵 / 秘密鍵ペアを使用したGoogle Cloud IoT Core。
- [ ] プロジェクト全体のSSH 認証鍵を備えたGoogle Compute Engine。
- [ ] 特定のSSH 認証鍵を持つGoogle Compute Engine。

  - IoTにはIoT Core
  - IoT Coreは認証に公開鍵 / 秘密鍵のペアを使う


37. この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winのセキュリティチームは、Google Cloud Platform（GCP）上の運用仮想マシン（VM）への外部 SSH アクセスを無効しました。
運用チームはVMをリモートで管理し、Docker コンテナのビルドとプッシュし、Google Cloud Storage オブジェクトの管理を行う必要があります。
セキュリティチームは、運用チームに何をするべきでしょうか？

- [x] 運用エンジニアにGoogle Cloud Shell へのアクセス権を付与します。
- [ ] GCP へのVPN 接続を構成し、GCP のVMへのSSH アクセスを許可します。
- [ ] 運用エンジニアがタスクを実行する必要がある場合に、GCP のVMへの一時的なSSH アクセスを許可する新しいアクセス要求プロセスを開発します。
- [ ] 開発チームにAPI サービスを構築して、運用チームが特定のリモート プロシージャ コール（RPC）を実行してタスクを実行できるようにします。

  - CloudShellにはgcloudコマンドとかgsutilがあらかじめインストールされてるから、Dockerコンテナのビルドプッシュができて、CloudStorageオブジェクト管理もできる。
  - Docker,CloudStorageを使えないので、B,C,DはNG？
  - RPC:Remote Procedure Call 関数とかを、リモートのサーバに処理させて、結果だけもらうような呼び出しの事


38.  ★この質問については、Dress4Win のケーススタディを参照してください。Dress4Winでは、運用エンジニアが、データベースのバックアップ ファイルのコピーをリモートでアーカイブするための低コストのソリューションを作成しようとしています。
データベース ファイルは、現在のデータセンターに格納されている圧縮 tar ファイルです。
何をするべきでしょうか？

- [x] gsutil を使用してcron スクリプトを作成し、ファイルをColdline Storage バケットにコピーします。
- [ ] gsutil を使用してcron スクリプトを作成し、ファイルをRegional Storage バケットにコピーします。
- [ ] Google Cloud Storage Transfer Service ジョブを作成して、ファイルをColdline Storage バケットにコピーします。
- [ ] Google Cloud Storage Transfer Service ジョブを作成して、ファイルをRegional Storage バケットにコピーします。

  - Cloud Storage Transfer Service は、ほかのクラウド(AWS S3とか)からデータを持ってくるときに利用する
  - RegionalよりColdLineのほうが安い


39. この質問については、Dress4Win のケーススタディを参照してください。Dress4Winは、アプリケーション サーバで使用するマシンタイプの導入を検討しています。
何をするべきでしょうか？

- [ ] オンプレミスの物理ハードウェア コアとRAMをパブリック クラウド内の最も近いマシンタイプにマッピングします。
- [ ] Dress4Winには、CPUに対するRAMの比率が最も高いマシンタイプにアプリケーション サーバを導入することをお勧めします。
- [x] Dress4Winには、最小のインスタンスを使用して本番環境に導入し、時間をかけて監視し、目的のパフォーマンスに達するまでマシンタイプをスケールアップすることをお勧めします。
- [ ] アプリケーション サーバの仮想マシンに関連付けられた仮想コアとRAMの数を特定し、パブリック クラウド内のカスタム マシンタイプに合わせてパフォーマンスを監視し、目的のパフォーマンスに達するまでマシンタイプをスケールアップします。

  - マシンタイプはコストパフォーマンスを考えないといけないから、段階的にスケールアップしていくほうがいいので、A,BはNG
  - 最初からオンプレ環境と同じコア数とかRAM数で構築すると、もしかしたらそれ以下でもOKかもしれないので、最小構成から少しずつスケールアップするほうがいいのでCが〇


40. この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winのパブリック クラウドへの移行計画の一環として、トラフィック負荷の急増に対処できるようにロギングとモニタリングの管理システムをセットアップしたいと考えています。
Dress4Winが希望する保証項目：

インフラストラクチャは、1日の使用量の増減を処理するためにスケールアップおよびスケールダウンが必要になったときの通知。
アプリケーションがエラー報告したときの管理者に自動通知。
集約されたログをフィルタリングして、多くのホストでアプリケーションの一部をデバッグ。
Google StackDriver のどの機能を使うべきか?

- [ ] Logging、Alerts、Insights、Debug。
- [ ] Monitoring, Trace, Debug, Logging。
- [ ] Monitoring, Logging, Alerts, Error Reporting。
- [x] Monitoring, Logging, Debug, Error Report。

  - Alerts,InsightsというサービスはないのでA,CはNG これはやらしい
  - 旧StackDriver:Monitoring,Logging,Error Reporting,Debugger,Trace,Profiler
  - スケールアップダウンが必要になった時の通知:Monitoring
  - エラー報告した時の自動通知:Error Reporting
  - ログをフィルタリング:Logging
  - アプリケーションの一部をデバッグ:Debug
  - Trace:リクエストがアプリケーションを通じて伝播される様子をトレースする
    - ボトルネックを検出
  - Debugger:アプリを停止せずにデバッグ
  - Profiler:CPUとかの状態をInteractiveでGraficalな画面で監視できる
    - ボトルネックを検出


41. この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winは、一部のアプリケーションをそのまま迅速に正常にデプロイすることにより、パブリック クラウドへのアプリケーションのデプロイに精通したいと考えています。
何を行えばよいでしょうか？

- [ ] パブリック クラウドへの最初の移行として、外部に依存する自己完結型アプリケーションを特定します。
- [ ] 内部的に依存しているエンタープライズアプリケーションを特定し、パブリック クラウドへの最初の移行します。
- [x] 社内データベースをパブリック クラウドに移行し、オンプレミス アプリケーションへのリクエストを継続的に処理します。
- [ ] メッセージ キュー サーバをパブリック クラウドに移動し、オンプレミス アプリケーションへのリクエストの処理を続行します。

  - 依存しているアプリを最初にパブリッククラウドに移すのはよくないからA,BはNG?
  - メッセージキューだけパブリッククラウドに移したらリクエストの処理ができない？


42. この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winは、オンプレミスのMySQL 環境をパブリック クラウドに移行する方法についてアドバイスを求めています。
移行中の自社運用ソリューションへのダウンタイムとパフォーマンスの影響を最小限に抑えたいと考えています。
何をすれば良いでしょうか？

- [ ] オンプレミス MySQL マスターサーバのダンプ（dump）ファイルを作成し、シャットダウンしてパブリック クラウド環境にアップロードし、新しいMySQL クラスタにロードします。
- [x] パブリック クラウド環境でMySQL レプリカサーバ/スレーブを設定し、カットオーバまでオンプレミスのMySQL マスターサーバから非同期レプリケーション用に構成します。
- [ ] パブリック クラウドに新しいMySQL クラスタを作成し、オンプレミスとパブリック クラウドの両方のMySQL マスターへの書き込みを開始するようにアプリケーションを構成し、カットオーバー時に元のクラスタを破棄します。
- [ ] MySQL レプリカサーバのダンプ ファイルをパブリック クラウド環境に作成してGoogle Cloud Datastore にロードし、カットオーバ時にGoogle Cloud Datastore に対して読み取り/書き込みを行うようにアプリケーションを構成します。

  - ダウンタイムを作ってしまうのでAはNG
  - MySQLを使えるのはCloudSQLだけなのでDもNG
  - パブリックにマスター作って、2重にマスター書き込みするようにしたら、アプリケーションのパフォーマンスに影響が出るのでCはNG


43. この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winは、従来のサービスのいくつかについて、Google Stackdriver で新しい稼働時間チェックを設定しました。
Stackdriver ダッシュボードは、サービスが正常であると報告していません。
何をするべきでしょうか？

- [ ] すべての従来のWebサーバにStackdriver エージェントをインストールします。
- [x] Google Cloud Platform Console で、アップタイムサーバのIP アドレスのリストをダウンロードし、インバウンド ファイアウォール ルールを作成します。
- [ ] 値がGoogleStackdriverMonitoring-UptimeChecks と一致したときにUser-Agent HTTP ヘッダーを通過するようにロードバランサーを構成します。（https://cloud.google.com/monitoring）
- [ ] 値がGoogleStackdriverMonitoring-UptimeChecks と一致する場合、user-Agent HTTPヘッダーを含むリクエストを許可するように従来のWebサーバーを設定します。(https://cloud.google.com/monitoring)

  - 稼働時間チェック:サービスが利用可能かどうかのチェック
  - 稼働時間チェックは、様々な場所からサービスにアクセスしてみて利用可能かどうかを判定するので、ファイアウォールの影響を受ける
  - なんで、対応するポートからのアクセス設定するファイアウォールルールを作成しないといけない(80,8080,443,その他TCP)
  - すなわち、稼働時間チェックは、外部IPを持つインスタンスしか設定できない
  - HTTP,HTTPS,TCPを選択できる
  - アップタイムサーバからの稼働時間チェックリクエストには、以下の2つの情報が含まれている
  - ip : アップタイムサーバ群のうちの一つのIP
  - User-Agent : GoogleStackdriverMonitoring-UptimeChecks(https://cloud.google.com/monitoring)←この値が常に入ってる
  - ホワイトリストに登録されてるIP(アップタイムサーバのIP)のリストをダウンロードして、そのIPからの対象ポートのアクセスを許可するのでBが〇
  - エージェントをインストールしなくても、CPU使用率とかロードアベレージとか簡単な値は取得できるので、エージェントをインストールしても問題は解決しないのでAはNG
  - ロードバランサーを作成しても、ファイアウォールで許可されてなかったらアクセスできないので、CはNG
  - webサーバでアクセス許可設定をしても、ファイアウォールルールで許可しないとGCPのリソースにはアクセスできないのでDもNG


44.  この質問については、Dress4Win のケーススタディを参照してください。
新しいアプリケーション体験の一環として、Dress4Wmでは顧客が自分の画像をアップロードできます。
顧客は、これらのイメージを表示できるユーザーを独占的に管理できます。
顧客は、最小限の待ち時間で画像をアップロードでき、ログイン時にメイン アプリケーション ページに画像をすばやく表示できる必要があります。
どの構成を使用するべきでしょうか？

- [x] Google Cloud Storage バケットに画像ファイルを保存します。Google Cloud Datastore を使用して、各顧客のIDとその画像ファイルをマッピングするメタデータを維持します。
- [ ] Google Cloud Storage バケットに画像ファイルを保存します。顧客の一意のIDを含むGoogle Cloud Storageのアップロードされた画像にカスタムメタデータを追加します。
- [ ] 分散ファイルシステムを使用して、顧客の画像を保存します。ストレージのニーズが増えたときには、永続ディスクやノードを追加します。各ファイルの所有者属性を設定する一意のIDを各顧客に割り当て、画像のプライバシーを確保します。
- [ ] 分散ファイルシステムを使用して、顧客の画像を保存します。ストレージのニーズが増えたときには、永続ディスクやノードを追加します。Google Cloud SQL データベースを使用して、各顧客のIDを画像ファイルにマッピングするメタデータを維持します。

  - BlobデータはCloud Storageに保存するのでC,DはNG
  - DatastoreはNoSQLなので処理が高速！これを使わない手はないのでAが〇
  - アプリケーションで画像を呼び出すときは、データベースサービスを使うのがいいと思う


45.  この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winには、エンドポイントの100％をカバーするエンドツーエンドのテストがあります。
パブリック クラウドへの移行によって新しいバグが発生しないようにしたいと考えています。
停止を防ぐために開発者はどのような追加のテスト方法を採用するべきですか？

- [ ] アプリケーション コードでGoogle Stackdriver Debugger を有効にして、コード内のエラーを表示する必要があります。
- [ ] パブリック クラウドのステージング環境にユニット（単体）テストと実稼働規模の負荷テストを追加する必要があります。
- [ ] パブリック クラウドのステージング環境でエンドツーエンドのテストを実行して、コードが意図したとおりに機能しているかどうかを判断する必要があります。
- [ ] カナリアテストを追加して、開発者が新しいリリースが遅延に与える影響を測定できるようにします。

  - カナリアテスト         : 新しい機能を一部のユーザだけにちょいだししてうまくリリースできるか試すテスト
  - ABテスト               : 新しい機能と従来の機能を1度に同時に本番リリースして、ユーザをランダムに振り分けて、新機能の評価を探るテスト
  - ブルーグリーンデプロイ : 本番環境ごともう一個作成して、そっちに新機能をリリースして、完了したら、LBとかで向き先を新しい本番に移す リリースがうまくできるか試すテスト
  - エンドポイントの100％をカバーするエンドツーエンドテストは、すべてのサーバに対してのテストってことかな
  - 単体(ユニット)テスト   : プログラムを構成する個々のユニット(関数とかメソッド)が、正しく動いているかどうかを調査するテスト
  - クラウドへの移行によって、”新たな”バグが発生しないようにしたいとあるので、アプリケーションのコードをデバッグしても、移行しただけなのでエラーは出ないはずなのでAはNG
  - エンドツーエンドテストのみだと、新しいバグは見つからない可能性が高いのでCはNG
  - カナリアテストでうまくリリースできるかどうかを調査しても、コードのバグは発見できないのでDもNG
  - 移行後に単体テストと負荷テストを追加することで、クラウドに移したことによる新しいバグを発見できるかもしれないので、Bが〇


46.   この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winの販売記録と税記録は、監査人が少なくとも10年間は見ることはありません。
最優先事項は、コストの最適化です。
どのGoogle プロダクトを選択するべきでしょうか？

- [x] データはGoogle Cloud Coldline Storage に保存し、gsutil でデータにアクセスします。
- [ ] データはGoogle Cloud Nearline Storage に保存し、gsutil でデータにアクセスします。
- [ ] データはアメリカまたはヨーロッパ リージョンを指定し、Google Bigtabte に保存し、gcloud でデータにアクセスします。
- [ ] データはGoogle BigQuery に保存し、マネージド インスタンス グループ（MIG）内のWebサーバ クラスタでデータにアクセスします。Google Cloud SQL に、データを格納するために2つの異なるリージョンにミラーリングを行い、マネージド インスタンス グループ（MIG）内のRedis クラスタにミラーリングされたデータにアクセスします。

  - コストの最適化はColdLine


47.   この質問については、Dress4Win のケーススタディを参照してください。
現在のDress4winのシステム アーキテクチャは、1つのデータセンターに配置されているため、一部のお客様によってはレーテンシーが高くなることがあります。
パブリック クラウドでのパフォーマンスの将来の評価と最適化の時点で、Dresses4winは、Google Cloud Platform のシステムアーキテクチャを複数の場所に配布したいと考えています。
どのアプローチを採用するべきですか？

- [x] リージョン マネージド インスタンス グループとグローバル負荷分散を使用してパフォーマンスを向上させます。リージョン マネージド インスタンス グループは、トラフィックに基づいて各地域のインスタンスを個別に拡大できるためです。
- [ ] 運用チームが管理する仮想マシンのより近いグループにリクエストを転送する仮想マシンのセットでグローバル負荷分散を使用します。
- [ ] リージョン マネージド インスタンス グループとグローバル負荷分散を使用して、異なるリージョンのゾーン間で自動フェールオーバーを提供することにより、信頼性を向上させます。
- [ ] 個別のマネージド インスタンス グループの一部として、リクエストをより近い仮想マシンのグループに転送する一連の仮想マシンでグローバル負荷分散を使用します。

  - システムアーキテクチャを複数の場所に配布したいとあるので、マネージドインスタンスグループを複数リージョンに配置して、グローバルLBを使う
  - 自動フェイルオーバーで信頼性を向上することはレイテンシと関係ないのでCはNG


48.  この質問については、Dress4Win のケーススタディを参照してください。
Dress4Winは、既存の使用パターンを反映したデータとトラフィックの対応する成長とともに、1年でそのサイズの10倍に成長すると予想されます。
CIOは、今後6ヵ月以内に運用しているインフラストラクチャをパブリック クラウドに移行するという目標を設定しました。
アプリケーションに大きな変更することなく、この成長に合わせて拡張し、ROI を最大化するようにソリューションをどのように構成すれば良いでしょうか？

- [ ] Web アプリケーション レイヤーをGoogle App Engine に、MySQL をGoogle Cloud Datastore に、NASをGoogle Cloud Storage に移行します。RabbitMQ をデプロイし、Google Cloud Deployment Manager を使ってHadoop サーバをデプロイします。
- [ ] RabbitMQ をGoogle Cloud Pub/Subに、Hadoop をGoogle BigQueryに、NASをGoogle Compute Engine とPersistent Disk ストレージに移行します。Tomcat をデプロイして、Google Cloud Deployment Manager を使ってNginx をデプロイします。
- [ ] Tomcat およびNginx 用のマネージド インスタンス グループを実装します。MySQL をGoogle Cloud SQL に、RabbitMQ をGoogle Cloud Pub/Sub に、Hadoop をGoogle Cloud Dataproc に、NAS をGoogle Compute Engine とPersistent Disk ストレージに移行します。
- [x] Tomcat およびNginx 用のマネージド インスタンス グループを実装します。MySQL をGoogle Cloud SQL に、RabbitMQ をGoogle Cloud Pub/Sub に、Hadoop をGoogle Cloud Dataproc に、NASをGoogle Cloud Storage に移行します。

  - WebサーバはGCEインスタンスグループ、MySQLはCloudSQL、RabbitMQはPub/Sub、HadoopはDataproc、NASはCloudStorageを使うのでDが〇
  - GCE       : 仮想マシンのマネージド
  - Cloud SQL : MySQL,PostgreSQLのマネージド
  - Pub/Sub   : RabbitMQなどPub/Subサービスのマネージド
  - Dataproc  : Hadoop,Sparkのマネージド
  - Storage   : NASとしても使える(FUSE)


49. この質問については、Dress4Win のケーススタディを参照してください。
特定のビジネス要件を考慮して、Webおよびトランザクション データレイヤーの導入をどのように自動化すれば良いでしょうか？

- [x] Google Cloud Deployment Manager を使用して、Nginx とTomcat をGoogle Compute Engine にデプロイします。 MySQL を置き換えるCloud SQL サーバをデプロイします。Google Cloud Deployment Manager を使用してJenkins を展開します。
- [ ] Google Cloud Platform（GCP）Marketplace を使用してNginx とTomcat を展開します。GCP Marketplace を使用してMySQL サーバを展開します。Google Cloud Deployment Manager スクリプトを使用して、Jenkins をGoogle Compute Engine にデプロイします。
- [ ] Nginx とTomcat をGoogle App Engine に移行します。Google Cloud Datastore サーバをデプロイして、高可用性構成のMySQL サーバを置き換えます。GCP Marketplace を使用してJenkins をGoogle Compute Engine にデプロイします。
- [ ] Nginx とTomcat をGoogle App Engine に移行します。GCP Marketplace を使用してMySQL サーバを展開します。GCP Marketplace を使用してJenkins をGoogle Compute Engine にデプロイします。

  - インフラの自動化:Terraform,Ansible,Puppet,Chef,DeploymentManager(GCP),CloudFormation(AWS)
  - MySQLにはCloudSQLを使ったほうがよくて、GCEのマーケットプレイスMySQLインスタンスを使うのは好ましくないのでB,DはNG
  - MySQLの置き換えにDatastoreは使えないのでCはNG


50. この質問については、Dress4Win のケーススタディを参照してください。
どのコンピューティングサービスをそのまま移行を行うと、パブリック クラウドでのパフォーマンスのために最適化されたアーキテクチャになりますか？

- [x] Google App Engine スタンダード環境を使用して展開されたWebアプリケーション。
- [ ] 管理されていないインスタンス グループを使用して展開されたRabbitMQ。
- [ ] 高可用性モードでGoogle Cloud Dataproc Regional を使用して展開されたHadoop/Spark。
- [ ] ジェンキンス、監視、要塞ホスト、カスタムマシンタイプに展開されたセキュリティスキャナーサービス。

  - WebアプリケーションをAppEngineに移行すると一番最適化されてるのでAが〇
  - RabbitMQはPub/Subに移行するべきなのでNG
  - Hadoop/SparkはDataprocでいいからCも〇やと思うけどなぁ
  - パブリッククラウド関係ないのでDはNG


51. この質問については、Dress4Win のケーススタディを参照してください。
監査中に法令に準拠するためには、Dress4WinはGoogle Cloud 上のリソース構成やメタデータを変更するすべての管理アクションを洞察できなければなりません。
何をするべきでしょうか？

- [ ] Stackdriver Trace を使用して、トレースリスト分析を作成します。
- [ ] Stackdriver Monitoring を使用して、プロジェクトのアクティビティに関するダッシュボードを作成します。
- [ ] すべてのプロジェクトでCloud Identity-Aware Proxy を有効にし、管理者（Administrators）グループをメンバーとして追加します。
- [x] Google Cloud Platform Console のアクティビティページとStackdriver Logging を使用して、必要な洞察を提供します。

  - CloudAuditLog : Loggingサービスの一つのサービス。ユーザとかサービスアカウントのリソース制御などの管理アクションを監査できる
  - CloudAuditLogはアクティビティページ(ベルマーク)でも見れる


52.  ★この質問については、Dress4Win のケーススタディを参照してください。Dress4WinのGoogle Cloud Storage に保存されたデータのセキュリティに責任があります。
既にGoogle グループのセットを作成し、適切なユーザーをそれらのグループに割り当てています。
Google ベストプラクティスに従って、ビジネス要件と技術的要件を満たすために最も単純な設計を実装する必要があります。
何をするべきでしょうか？

- [ ] セキュリティ要件を実施するために、作成したGoogl eグループにIAMのカスタムの役割を割り当てます。Google Cloud Storage にファイルを保存するときに、顧客が用意した暗号鍵でデータを暗号化します。
- [ ] セキュリティ要件を実施するために、作成したGoogle グループにIAMのカスタムの役割を割り当てます。Google Cloud Storage にファイルを保存する前に、デフォルトのストレージ暗号化を有効します。
- [x] セキュリティ要件を実施するために、作成したGoogle グループに定義済みのIAMの役割を割り当てます。Google Cloud Storage にファイルを保存するときに、Google のデフォルトの暗号化を保存時に使用します。
- [ ] セキュリティ要件を実施するために、作成したGoogle グループに定義済みのIAMの役割を割り当てます。Google Cloud Storage にファイルを保存する前に、デフォルトのGoogle Cloud KMS の暗号鍵が設定されていることを確認します。

  - もっとも単純な構成は、事前定義済みのIAMロールを割り当てて、Googleのデフォルト暗号鍵を使うものなので、Cが〇


53.  ★この質問については、Dress4Win のケーススタディを参照してください。
ソリューションを移行する前に、オンプレミス アーキテクチャがビジネス要件を満たしていることを確認する必要があります。
オンプレミスのアーキテクチャをどのように変更する必要がありますか？

- [ ] RabbitMQ をGoogle Cloud Pub / Subに置き換えます。
- [ ] MySQL をGoogle Cloud SQL for MySQL でサポートされているv5.7にダウングレードします。
- [x] 事前定義されたGoogle Compute Engine マシンタイプに一致するようにコンピューティング リソースのサイズを変更します。
- [ ] マイクロサービスをコンテナ化し、Google Kubernetes Engine でホストします。

  - 移行する前段階の話なので、A,DはNG
  - 既存環境もMySQL5.7なのでダウングレードにならないBはNG